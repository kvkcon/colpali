{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinaai/miniconda3/envs/rerank_vlm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s]\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/data/hf_models/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda:6\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = '''Give me a short introduction to 回收显存.\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from colpali_engine.models import ColQwen2, ColQwen2Processor\n",
    "\n",
    "# 加载基础模型\n",
    "base_model = ColQwen2.from_pretrained(\n",
    "    \"/data/hf_models/colqwen2-base\",  \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:7\"\n",
    ")\n",
    "# 加载 LoRA 配置\n",
    "peft_model_id = \"/data/hf_models/colqwen2-v0.1\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# 创建 PeftModel\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "model.eval()\n",
    "\n",
    "# 加载处理器\n",
    "processor = ColQwen2Processor.from_pretrained(peft_model_id)\n",
    "\n",
    "# 遍历前5个样本\n",
    "for i in range(5):\n",
    "    sample = ds[i]\n",
    "    \n",
    "    # 直接使用数据集中的图像对象\n",
    "    image = sample['image']\n",
    "    # query = sample['query']\n",
    "    query = ds[0]['query']\n",
    "    \n",
    "    # 处理输入\n",
    "    batch_images = processor.process_images([image]).to(model.device)\n",
    "    batch_queries = processor.process_queries([query]).to(model.device)\n",
    "    \n",
    "    # 推理\n",
    "    with torch.no_grad():\n",
    "        image_embeddings = model(**batch_images)\n",
    "        query_embeddings = model(**batch_queries)\n",
    "    \n",
    "    scores = processor.score_multi_vector(query_embeddings, image_embeddings)\n",
    "    \n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Score: {scores.item()}\")\n",
    "'''\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你提供的代码片段主要用于加载一个基于ColQwen2的模型，并对其进行微调以适应特定任务，例如图像检索。在这个过程中，代码还展示了如何处理图像和查询文本，以及如何计算它们之间的相似度得分。\\n\\n关于“回收显存”（回收显存资源），在深度学习框架中，特别是在大规模模型推理时，有效管理显存是非常重要的。回收显存通常涉及以下几个方面：\\n\\n1. **释放不必要的张量**：在完成计算后，可以使用`del`关键字删除不再需要的张量，这样可以释放显存。\\n2. **清空缓存**：可以通过调用`torch.cuda.empty_cache()`来清空GPU缓存，这有助于释放未使用的显存。\\n3. **调整批处理大小**：减少批处理的大小可以降低每次迭代所需的显存量。\\n4. **使用更高效的数据结构**：选择更节省内存的数据结构或方法可以减少显存使用。\\n\\n下面是如何在你的代码中添加显存回收的部分示例：\\n\\n```python\\nimport torch\\nfrom peft import PeftModel, PeftConfig\\nfrom colpali_engine.models import ColQwen2, ColQwen2Processor\\n\\n# 加载基础模型\\nbase_model = ColQwen2.from_pretrained(\\n    \"/data/hf_models/colqwen2-base\",\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"cuda:7\"\\n)\\n# 加载 LoRA 配置\\npeft_model_id = \"/data/hf_models/colqwen2-v0.1\"\\nconfig = PeftConfig.from_pretrained(peft_model_id)\\n\\n# 创建 PeftModel\\nmodel = PeftModel.from_pretrained(base_model, peft_model_id)\\nmodel.eval()\\n\\n# 加载处理器\\nprocessor = ColQwen2Processor.from_pretrained(peft_model_id)\\n\\n# 遍历前5个样本\\nfor i in range(5):\\n    sample = ds[i]\\n    \\n    # 直接使用数据集中的图像对象\\n    image = sample[\\'image\\']\\n    # query = sample[\\'query\\']\\n    query = ds[0][\\'query\\']\\n    \\n    # 处理输入\\n    batch_images = processor.process_images([image]).to(model.device)\\n    batch_queries = processor.process_queries([query]).to(model.device)\\n    \\n    # 推理\\n    with torch.no_grad():\\n        image_embeddings = model(**batch_images)\\n        query_embeddings = model(**batch_queries'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rerank_vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
